{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Loading the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the dog dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "#Using the load_files function from the scikit-learn library to\n",
    "#populate the following variables\n",
    "#train, valid and test_files are numpy arrays containing file paths to images\n",
    "#train, valid and test_targets are numpy arrays containing onehot-encoded labels\n",
    "#dog_names is a list os strings containing the dog breed names\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133) #133 breeds\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "\n",
    "dog_names = [item[20:-1] for item in sorted(glob('dogImages/train/*/'))]\n",
    "\n",
    "#print some statistics\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the humans dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13233 total human images.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "\n",
    "human_files = np.array(glob('lfw/*/*'))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "#print some statistics\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Human detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Face detection with opencv Haar feature-based cascade  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "#load the classifier into a variable\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "#Given an image path this function whil return true if a face was\n",
    "#detected in the image and false otherwise\n",
    "def HaarCascade_faceDetector(img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        faces = face_cascade.detectMultiScale(gray_img)\n",
    "        return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Face detection with opencv deep learning approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now it's time to test the two approaches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing in the first 100 images of humans and dogs\n",
      "98 human faces were detected in the first 100 images of humans\n",
      "10 human faces were detected in the first 100 images of dogs\n"
     ]
    }
   ],
   "source": [
    "human_files_short = human_files[:100]\n",
    "dog_files_short = train_files[:100]\n",
    "\n",
    "count_humans = 0\n",
    "count_dogs = 0\n",
    "\n",
    "for path in human_files_short:\n",
    "    if(HaarCascade_faceDetector(path)):\n",
    "        count_humans += 1\n",
    "\n",
    "for path in dog_files_short:\n",
    "    if(HaarCascade_faceDetector(path)):\n",
    "        count_dogs += 1\n",
    "\n",
    "print('Testing in the first 100 images of humans and dogs')\n",
    "print('%d human faces were detected in the first 100 images of humans' % count_humans)\n",
    "print('%d human faces were detected in the first 100 images of dogs' % count_dogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Detecting dogs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using ResNet50 trained in the ImageNet dataset to detect if there is a dog in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "\n",
    "ResNet50_model = ResNet50(weights='imagenet')\n",
    "\n",
    "#Keras CNNs require a 4D tensor as input in the form\n",
    "#      (nb_samples, rows, columns, channels)\n",
    "#So we use the path_to_tensor function to convert the image into (1, 224, 224, 3)\n",
    "#And the paths_to_tensor function to make all tensors together (nb_samples, 224, 224, 3)\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    #read and resize the image\n",
    "    img = image.load_img(img_path, target_size=(224,224))\n",
    "    #convert image to a 3D tensor with shape (224, 224, 3)\n",
    "    tensor_3d = image.img_to_array(img)\n",
    "    #convert the 3D tensor to a 4D tensor with shape (1, 224, 224, 3)\n",
    "    return np.expand_dims(tensor_3d, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "#Getting the 4D tensor ready to the ResNet50 requires some additional processing\n",
    "#like converting the RGB image to BGR and some normalization steps that\n",
    "#the preprecess function from keras will make for us\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))\n",
    "\n",
    "#now we can create the dog detector function\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing the dog detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs found in Human dataset: 1\n",
      "dogs found in dogs dataset: 100\n"
     ]
    }
   ],
   "source": [
    "count_humans = 0\n",
    "count_dogs = 0\n",
    "\n",
    "for path in human_files_short:\n",
    "    if(dog_detector(path)):\n",
    "        count_humans += 1\n",
    "\n",
    "for path in dog_files_short:\n",
    "    if(dog_detector(path)):\n",
    "        count_dogs += 1\n",
    "\n",
    "print('dogs found in Human dataset:', count_humans)\n",
    "print('dogs found in dogs dataset:', count_dogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Creating the CNNs to classify Dog Breeds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Creating a CNN from scratch in Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6680/6680 [02:42<00:00, 41.08it/s]\n",
      "100%|██████████| 835/835 [00:20<00:00, 40.43it/s]\n",
      "100%|██████████| 836/836 [00:18<00:00, 44.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cfcv/Tensorflow/ML/local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1062: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 222, 222, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 220, 220, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 110, 110, 64)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 774400)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               99123328  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 133)               17157     \n",
      "=================================================================\n",
      "Total params: 99,159,877.0\n",
      "Trainable params: 99,159,877.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#defining the architecture\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=3, padding='valid', activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(Conv2D(filters=64, kernel_size=3, padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(133, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cfcv/Tensorflow/ML/local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:2548: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/cfcv/Tensorflow/ML/local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1123: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "#compiling the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/5\n",
      "6660/6680 [============================>.] - ETA: 5s - loss: 15.9186 - acc: 0.0102 Epoch 00000: val_loss improved from inf to 15.94437, saving model to saved_models/weights.best.from_scratch.keras.hdf5\n",
      "6680/6680 [==============================] - 1729s - loss: 15.9192 - acc: 0.0102 - val_loss: 15.9444 - val_acc: 0.0108\n",
      "Epoch 2/5\n",
      "6660/6680 [============================>.] - ETA: 4s - loss: 5.4909 - acc: 0.0137Epoch 00001: val_loss improved from 15.94437 to 4.78571, saving model to saved_models/weights.best.from_scratch.keras.hdf5\n",
      "6680/6680 [==============================] - 1639s - loss: 5.4890 - acc: 0.0136 - val_loss: 4.7857 - val_acc: 0.0204\n",
      "Epoch 3/5\n",
      "6660/6680 [============================>.] - ETA: 4s - loss: 4.2028 - acc: 0.1267Epoch 00002: val_loss improved from 4.78571 to 4.78155, saving model to saved_models/weights.best.from_scratch.keras.hdf5\n",
      "6680/6680 [==============================] - 1596s - loss: 4.2045 - acc: 0.1265 - val_loss: 4.7816 - val_acc: 0.0371\n",
      "Epoch 4/5\n",
      "6660/6680 [============================>.] - ETA: 4s - loss: 1.2421 - acc: 0.7303Epoch 00003: val_loss did not improve\n",
      "6680/6680 [==============================] - 1598s - loss: 1.2432 - acc: 0.7302 - val_loss: 7.2074 - val_acc: 0.0407\n",
      "Epoch 5/5\n",
      "6660/6680 [============================>.] - ETA: 4s - loss: 0.2700 - acc: 0.9533Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 1610s - loss: 0.2695 - acc: 0.9533 - val_loss: 9.2935 - val_acc: 0.0407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f65af77feb8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "epochs = 5\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.keras.hdf5',\n",
    "                              verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets,\n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 4.7847%\n"
     ]
    }
   ],
   "source": [
    "#Load the best model and test it\n",
    "model.load_weights('saved_models/weights.best.from_scratch.keras.hdf5')\n",
    "\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Using transfer learning to classify dogs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the bottleneck features of the Resnet to use to transfer learning\n",
    "bottleneck_features = np.load('bottleneck_features/DogResnet50Data.npz')\n",
    "train_RESNET50 = bottleneck_features['train']\n",
    "test_RESNET50 = bottleneck_features['test']\n",
    "valid_RESNET50 = bottleneck_features['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_2 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 133)               34181     \n",
      "=================================================================\n",
      "Total params: 558,725.0\n",
      "Trainable params: 558,725.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#print(train_RESNET50.shape)\n",
    "#train_RESNET50.shape = (6680, 1, 1, 2048)\n",
    "#To pass to our fully conected NN we will need only the shape of the flatten\n",
    "#vector and not the number of samples(6680)\n",
    "model_RNT = Sequential()\n",
    "model_RNT.add(GlobalAveragePooling2D(input_shape=train_RESNET50.shape[1:]))\n",
    "model_RNT.add(Dense(256, activation='relu'))\n",
    "model_RNT.add(Dropout(0.2))\n",
    "model_RNT.add(Dense(133, activation='softmax'))\n",
    "model_RNT.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling the model\n",
    "model_RNT.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/40\n",
      "6680/6680 [==============================] - 2s - loss: 1.0894 - acc: 0.7127 - val_loss: 0.9106 - val_acc: 0.7545\n",
      "Epoch 2/40\n",
      "6680/6680 [==============================] - 1s - loss: 0.8160 - acc: 0.7780 - val_loss: 0.7697 - val_acc: 0.7868\n",
      "Epoch 3/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.6585 - acc: 0.8181 - val_loss: 0.6972 - val_acc: 0.8036\n",
      "Epoch 4/40\n",
      "6680/6680 [==============================] - 1s - loss: 0.5774 - acc: 0.8434 - val_loss: 0.6518 - val_acc: 0.8012\n",
      "Epoch 5/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.5020 - acc: 0.8537 - val_loss: 0.6203 - val_acc: 0.8168\n",
      "Epoch 6/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.4376 - acc: 0.8775 - val_loss: 0.6033 - val_acc: 0.8240\n",
      "Epoch 7/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.3965 - acc: 0.8931 - val_loss: 0.5825 - val_acc: 0.8251\n",
      "Epoch 8/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.3628 - acc: 0.8982 - val_loss: 0.5621 - val_acc: 0.8299\n",
      "Epoch 9/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.3293 - acc: 0.9121 - val_loss: 0.5769 - val_acc: 0.8204\n",
      "Epoch 10/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.3047 - acc: 0.9177 - val_loss: 0.5518 - val_acc: 0.8251\n",
      "Epoch 11/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.2757 - acc: 0.9310 - val_loss: 0.5353 - val_acc: 0.8371\n",
      "Epoch 12/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.2608 - acc: 0.9341 - val_loss: 0.5405 - val_acc: 0.8347\n",
      "Epoch 13/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.2387 - acc: 0.9407 - val_loss: 0.5279 - val_acc: 0.8323\n",
      "Epoch 14/40\n",
      "6680/6680 [==============================] - 1s - loss: 0.2210 - acc: 0.9484 - val_loss: 0.5332 - val_acc: 0.8299\n",
      "Epoch 15/40\n",
      "6680/6680 [==============================] - 1s - loss: 0.2060 - acc: 0.9510 - val_loss: 0.5209 - val_acc: 0.8359\n",
      "Epoch 16/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.1941 - acc: 0.9564 - val_loss: 0.5246 - val_acc: 0.8263\n",
      "Epoch 17/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.1854 - acc: 0.9581 - val_loss: 0.5111 - val_acc: 0.8407\n",
      "Epoch 18/40\n",
      "6680/6680 [==============================] - 1s - loss: 0.1798 - acc: 0.9581 - val_loss: 0.5064 - val_acc: 0.8407\n",
      "Epoch 19/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.1654 - acc: 0.9651 - val_loss: 0.5086 - val_acc: 0.8419\n",
      "Epoch 20/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.1577 - acc: 0.9675 - val_loss: 0.5012 - val_acc: 0.8443\n",
      "Epoch 21/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.1508 - acc: 0.9662 - val_loss: 0.5107 - val_acc: 0.8443\n",
      "Epoch 22/40\n",
      "6680/6680 [==============================] - 1s - loss: 0.1430 - acc: 0.9693 - val_loss: 0.5110 - val_acc: 0.8371\n",
      "Epoch 23/40\n",
      "6680/6680 [==============================] - 1s - loss: 0.1356 - acc: 0.9713 - val_loss: 0.5085 - val_acc: 0.8359\n",
      "Epoch 24/40\n",
      "6680/6680 [==============================] - 1s - loss: 0.1260 - acc: 0.9738 - val_loss: 0.5081 - val_acc: 0.8359\n",
      "Epoch 25/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.1218 - acc: 0.9781 - val_loss: 0.5029 - val_acc: 0.8383\n",
      "Epoch 26/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.1154 - acc: 0.9787 - val_loss: 0.5064 - val_acc: 0.8491\n",
      "Epoch 27/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.1159 - acc: 0.9784 - val_loss: 0.5016 - val_acc: 0.8383\n",
      "Epoch 28/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.1097 - acc: 0.9805 - val_loss: 0.5028 - val_acc: 0.8431\n",
      "Epoch 29/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.1096 - acc: 0.9796 - val_loss: 0.5011 - val_acc: 0.8407\n",
      "Epoch 30/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.1044 - acc: 0.9801 - val_loss: 0.4982 - val_acc: 0.8407\n",
      "Epoch 31/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.0944 - acc: 0.9841 - val_loss: 0.5053 - val_acc: 0.8383\n",
      "Epoch 32/40\n",
      "6680/6680 [==============================] - 1s - loss: 0.0907 - acc: 0.9855 - val_loss: 0.5011 - val_acc: 0.8479\n",
      "Epoch 33/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.0944 - acc: 0.9840 - val_loss: 0.5018 - val_acc: 0.8455\n",
      "Epoch 34/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.0864 - acc: 0.9847 - val_loss: 0.5022 - val_acc: 0.8371\n",
      "Epoch 35/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.0825 - acc: 0.9859 - val_loss: 0.5021 - val_acc: 0.8443\n",
      "Epoch 36/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.0825 - acc: 0.9876 - val_loss: 0.4998 - val_acc: 0.8455\n",
      "Epoch 37/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.0764 - acc: 0.9880 - val_loss: 0.4946 - val_acc: 0.8407\n",
      "Epoch 38/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.0748 - acc: 0.9894 - val_loss: 0.5035 - val_acc: 0.8443\n",
      "Epoch 39/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.0733 - acc: 0.9898 - val_loss: 0.4991 - val_acc: 0.8479\n",
      "Epoch 40/40\n",
      "6680/6680 [==============================] - 2s - loss: 0.0698 - acc: 0.9913 - val_loss: 0.4989 - val_acc: 0.8491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f808cf9e438>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.RESNET50.sgd.hdg5')\n",
    "model_RNT.fit(train_RESNET50, train_targets,\n",
    "         validation_data=(valid_RESNET50, valid_targets),\n",
    "         epochs=40, batch_size=20, verbose=1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the best model\n",
    "model_RNT.load_weights('saved_models/weights.best.RESNET50.sgd.hdg5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 84.6890%\n"
     ]
    }
   ],
   "source": [
    "RESNET50_predictions = [np.argmax(model_RNT.predict(np.expand_dims(feature, axis=0))) for feature in test_RESNET50]\n",
    "\n",
    "test_accuracy = 100*np.sum(np.array(RESNET50_predictions)==np.argmax(test_targets, axis=1))/len(RESNET50_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the function to predict a breed of a dog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from extract_bottleneck_features import *\n",
    "\n",
    "def RESNET50_predict_breed(img_path):\n",
    "    bottleneck_feature = extract_Resnet50(path_to_tensor(img_path))\n",
    "    predicted_vector = model_RNT.predict(bottleneck_feature)\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Writing the algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Application(img_path):\n",
    "    #check if there is a dog in the image\n",
    "    if(dog_detector(img_path)):\n",
    "        breed = RESNET50_predict_breed(img_path)\n",
    "        return 'The predicted breed of this dog is:' + breed\n",
    "    elif(face_detector(img_path)):\n",
    "        human_breed = RESNET50_predict_breed(img_path)\n",
    "        return 'This humans look like a:' + human_breed\n",
    "    else:\n",
    "        return 'Dog nor human detected'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
