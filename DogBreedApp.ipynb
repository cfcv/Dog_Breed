{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Loading the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the dog dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "#Using the load_files function from the scikit-learn library to\n",
    "#populate the following variables\n",
    "#train, valid and test_files are numpy arrays containing file paths to images\n",
    "#train, valid and test_targets are numpy arrays containing onehot-encoded labels\n",
    "#dog_names is a list os strings containing the dog breed names\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133) #133 breeds\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "\n",
    "dog_names = [item[20:-1] for item in sorted(glob('dogImages/train/*/'))]\n",
    "\n",
    "#print some statistics\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the humans dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13233 total human images.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "\n",
    "human_files = np.array(glob('lfw/*/*'))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "#print some statistics\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Human detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Face detection with opencv Haar feature-based cascade  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "#load the classifier into a variable\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "#Given an image path this function whil return true if a face was\n",
    "#detected in the image and false otherwise\n",
    "def HaarCascade_faceDetector(img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        faces = face_cascade.detectMultiScale(gray_img)\n",
    "        return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Face detection with opencv deep learning approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now it's time to test the two approaches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing in the first 100 images of humans and dogs\n",
      "98 human faces were detected in the first 100 images of humans\n",
      "10 human faces were detected in the first 100 images of dogs\n"
     ]
    }
   ],
   "source": [
    "human_files_short = human_files[:100]\n",
    "dog_files_short = train_files[:100]\n",
    "\n",
    "count_humans = 0\n",
    "count_dogs = 0\n",
    "\n",
    "for path in human_files_short:\n",
    "    if(HaarCascade_faceDetector(path)):\n",
    "        count_humans += 1\n",
    "\n",
    "for path in dog_files_short:\n",
    "    if(HaarCascade_faceDetector(path)):\n",
    "        count_dogs += 1\n",
    "\n",
    "print('Testing in the first 100 images of humans and dogs')\n",
    "print('%d human faces were detected in the first 100 images of humans' % count_humans)\n",
    "print('%d human faces were detected in the first 100 images of dogs' % count_dogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Detecting dogs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
